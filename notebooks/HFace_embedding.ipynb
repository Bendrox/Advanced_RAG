{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0302e2de",
   "metadata": {},
   "source": [
    "## 1 - Open source embedding model exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c7755",
   "metadata": {},
   "source": [
    "Exploring here other \"open source\" embedding models. \n",
    "\n",
    "For this task we will use MTEB that compares 100+ text (and image) embedding models across 1000, languages depending on metrics, languages, tasks, and task types:\n",
    "\n",
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "MTEB classe les modèles en fonction de  8 types de tâches  : ￼\n",
    "\n",
    "1.\tBitext Mining : trouver des phrases correspondantes dans deux langues différentes.\n",
    "\n",
    "2.\tClassification : attribuer des catégories aux textes.\n",
    "\n",
    "3.\tClustering : regrouper des textes similaires.\n",
    "\n",
    "4.\tPair Classification : déterminer si deux textes sont similaires.\n",
    "\n",
    "5.\tReranking : ordonner une liste de textes en fonction de leur pertinence par rapport à une requête.\n",
    "\n",
    "6.\tRetrieval : retrouver des documents pertinents pour une requête donnée.\n",
    "\n",
    "7.\tSemantic Textual Similarity (STS) : mesurer la similarité sémantique entre deux textes.\n",
    "\n",
    "8.\tSummarization : évaluer la qualité des résumés générés automatiquement. ￼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12b6f9",
   "metadata": {},
   "source": [
    "Critères de choix pour le modèle:\n",
    "- Filtrer sur domaine juridique + langue Fr\n",
    "- Performances en terme de Retrieval\n",
    "- Nombre de tokens maximal à partir de 8191 en raison de la longeur de nos chunks (articles de loi dont la taille très variables)\n",
    "- Taile du modele (en prenant le meilleur modele qui répond a tous les critères mais aussi un modele de 1B ou 0.5B)\n",
    "\n",
    "Résultats de la selection en fonction des critères (classement en fonction des score sur Retreival):\n",
    "\n",
    "- Catégorie 1 - modèles propriétaires: \n",
    "\n",
    "1 - [voyage-3](https://blog.voyageai.com/2024/09/18/voyage-3/) est le meilleur arrive en premier mais modele proprio (score de 85 en retreival) \n",
    "\n",
    "2 - ensuite vient celui de google [gemini-embedding-exp-03-07](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) - (?B) -  max token 8192 - score global 74  - score retreival 67.71\n",
    "\n",
    "\n",
    "\n",
    "- Catégorie 2 - modèles open source: \n",
    "\n",
    "1 - [inf-retriever-v1](https://huggingface.co/infly/inf-retriever-v1) (7B )  - score 73.89 mais aucun score sur STS ! sa version 1B est plus interessante car drop non significatif dans les perfs (72.14 )  [inf-retriever-v1-1.5b](https://huggingface.co/infly/inf-retriever-v1-1.5b)\n",
    "\n",
    "2 - [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral) 7B  - max token 32768 - score 68.46\n",
    "\n",
    "3 - [snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0) - 0.5 B - max token 8192 - score 65\n",
    "\n",
    "En intégrant le reranking au retreival : \n",
    "\n",
    "1 - [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral) - score global 84\n",
    "\n",
    "2 - [snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0) 80 en modele leger (0.5)\n",
    "\n",
    "\n",
    "Remarque : Tous ces modèles semblent supérieurs aux perfs de [text-embedding-3-large](https://openai.com/index/new-embedding-models-and-api-updates/)  qui en retreival a un score de 59.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentence‑Transformers gère tout le reste\n",
    "!pip install --upgrade sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d7c8",
   "metadata": {},
   "source": [
    "## 2 - Test loading inf-retriever-v1-1.5b ( on m2 mcbook air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97600106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ab84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----- 1) Choix de l'appareil (MPS pour Mac M1/M2) -----\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d4db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4893634a79e4490ab4fc14b69f2fe0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b77bfd913aa4fa9b353a998a5f3a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/284 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abae3b2babfd4f23a88a0e2123b94a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/19.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc169f18f1ad4020b3f9e95e3e3e7c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/55.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d234b4541bba4ccfb05110a6d9571659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- 2) Chargement du modèle prêt à l'emploi -----\n",
    "model = SentenceTransformer(\n",
    "    \"infly/inf-retriever-v1-1.5b\",   # ↙ modèle E5 + pooling\n",
    "    device=device, \n",
    "    token=os.getenv(\"HF_token\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b3de4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3) Lecture du JSON source -----\n",
    "with open(\"/Users/oussa/Desktop/Github_perso/Advanced_RAG/data_chunks/chunks_aml_5_strat_2.json\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3090777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 4) Génération des embeddings -----\n",
    "for doc in docs:\n",
    "    emb = model.encode(\n",
    "        doc[\"page_content\"],\n",
    "        normalize_embeddings=True              # distance cosine déjà ok\n",
    "    ).tolist()                                 # .tolist() pour JSON\n",
    "\n",
    "    doc[\"embedding\"] = emb                    # ajoute le vecteur\n",
    "    # petite trace du modèle et de la dimension\n",
    "    doc.setdefault(\"metadata\", {})[\"embedding_model_name\"] = (\n",
    "        \"infly/inf-retriever-v1-1.5b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 5) Sauvegarde -----\n",
    "with open(\"documents_with_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅  Embeddings ajoutés dans documents_with_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664109a",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "- Working : ok\n",
    "- Result : None\n",
    "- Taking time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce75ff",
   "metadata": {},
   "source": [
    "## 2 - Testing the most efficient model : snowflake-arctic-embed-l-v2.0 (0.5 B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b4539",
   "metadata": {},
   "source": [
    " Even if this model is ranked at the 5th position it is the lighetest and most efficient one without a significant drop in performance (only 0.5 B param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab8f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0991c594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5777bf8b6c80483d91f2d680fdfec0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02a5febb88a4c2ba8224811d6d6e4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/203 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3165cf662476891429567d78e53b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/251k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b29fae12c0341bfaaabf84f07af7b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/784 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54182ca2b74f429395e327a57d89e95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90b3d824d0a4460b155a61a964776bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824c27d3c8fa45f7b0365872036bbb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6289acf10d7143fd80cce8785865f79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af17fd0bcd6d47aaa7d1c2a242aea813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_05B = SentenceTransformer(\n",
    "    \"Snowflake/snowflake-arctic-embed-l-v2.0\",   # ↙ modèle E5 + pooling\n",
    "    device=device, \n",
    "    token=os.getenv(\"HF_token\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dc24913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3) Lecture du JSON source -----\n",
    "with open(\"/Users/oussa/Desktop/Github_perso/Advanced_RAG/data_chunks/chunks_aml_5_strat_2.json\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 4) Génération des embeddings -----\n",
    "for doc in docs:\n",
    "    emb = model.encode(\n",
    "        doc[\"page_content\"],\n",
    "        normalize_embeddings=True              # distance cosine déjà ok\n",
    "    ).tolist()                                 # .tolist() pour JSON\n",
    "\n",
    "    doc[\"embedding\"] = emb                    # ajoute le vecteur\n",
    "    # petite trace du modèle et de la dimension\n",
    "    doc.setdefault(\"metadata\", {})[\"embedding_model_name\"] = (\n",
    "        \"Snowflake-arctic-emb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "826732cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Embeddings ajoutés dans documents_with_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "# ----- 5) Sauvegarde -----\n",
    "with open(\"/Users/oussa/Desktop/Github_perso/Advanced_RAG/data_chunks/snowflake-arctic_embedding_chunk_2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅  Embeddings ajoutés dans documents_with_embeddings.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Advanced_RAG)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
