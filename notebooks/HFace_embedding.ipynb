{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0302e2de",
   "metadata": {},
   "source": [
    "## 1 - Open source embedding model exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c7755",
   "metadata": {},
   "source": [
    "Exploring here other \"open source\" embedding models. \n",
    "\n",
    "For this task we will use MTEB that compares 100+ text (and image) embedding models across 1000, languages depending on metrics, languages, tasks, and task types:\n",
    "\n",
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "MTEB classe les modèles en fonction de  8 types de tâches  : ￼\n",
    "\n",
    "1.\tBitext Mining : trouver des phrases correspondantes dans deux langues différentes.\n",
    "\n",
    "2.\tClassification : attribuer des catégories aux textes.\n",
    "\n",
    "3.\tClustering : regrouper des textes similaires.\n",
    "\n",
    "4.\tPair Classification : déterminer si deux textes sont similaires.\n",
    "\n",
    "5.\tReranking : ordonner une liste de textes en fonction de leur pertinence par rapport à une requête.\n",
    "\n",
    "6.\tRetrieval : retrouver des documents pertinents pour une requête donnée.\n",
    "\n",
    "7.\tSemantic Textual Similarity (STS) : mesurer la similarité sémantique entre deux textes.\n",
    "\n",
    "8.\tSummarization : évaluer la qualité des résumés générés automatiquement. ￼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12b6f9",
   "metadata": {},
   "source": [
    "Critères de choix pour le modèle:\n",
    "- Filtrer sur domaine juridique + langue Fr\n",
    "- Performances en terme de Retrieval\n",
    "- Nombre de tokens maximal à partir de 8191 en raison de la longeur de nos chunks (articles de loi dont la taille très variables)\n",
    "- Taile du modele (en prenant le meilleur modele qui répond a tous les critères mais aussi un modele de 1B ou 0.5B)\n",
    "\n",
    "Résultats de la selection en fonction des critères (classement en fonction des score sur Retreival):\n",
    "\n",
    "- Catégorie 1 - modèles propriétaires: \n",
    "\n",
    "1 - [voyage-3](https://blog.voyageai.com/2024/09/18/voyage-3/) est le meilleur arrive en premier mais modele proprio (score de 85 en retreival) \n",
    "\n",
    "2 - ensuite vient celui de google [gemini-embedding-exp-03-07](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) - (?B) -  max token 8192 - score global 74  - score retreival 67.71\n",
    "\n",
    "\n",
    "\n",
    "- Catégorie 2 - modèles open source: \n",
    "\n",
    "1 - [inf-retriever-v1](https://huggingface.co/infly/inf-retriever-v1) (7B )  - score 73.89 mais aucun score sur STS ! sa version 1B est plus interessante car drop non significatif dans les perfs (72.14 )  [inf-retriever-v1-1.5b](https://huggingface.co/infly/inf-retriever-v1-1.5b)\n",
    "\n",
    "2 - [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral) 7B  - max token 32768 - score 68.46\n",
    "\n",
    "3 - [snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0) - 0.5 B - max token 8192 - score 65\n",
    "\n",
    "En intégrant le reranking au retreival : \n",
    "\n",
    "1 - [SFR-Embedding-Mistral](https://huggingface.co/Salesforce/SFR-Embedding-Mistral) - score global 84\n",
    "\n",
    "2 - [snowflake-arctic-embed-l-v2.0](https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0) 80 en modele leger (0.5)\n",
    "\n",
    "\n",
    "Remarque : Tous ces modèles semblent supérieurs aux perfs de [text-embedding-3-large](https://openai.com/index/new-embedding-models-and-api-updates/)  qui en retreival a un score de 59.27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee3a6d",
   "metadata": {},
   "source": [
    "Deux choix possible au regard du drop dans les perfs : \n",
    "- inf-retreiver-v1 (7B) en production endpoint sur HF \n",
    "- ou tenter le inf-retreiver-v1 en 1.5B sinon le modele de snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentence‑Transformers gère tout le reste\n",
    "!pip install --upgrade sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a192a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d7c8",
   "metadata": {},
   "source": [
    "## 2 - Test loading inf-retriever-v1-1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97600106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ab84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----- 1) Choix de l'appareil (MPS pour Mac M1/M2) -----\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d4db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4893634a79e4490ab4fc14b69f2fe0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b77bfd913aa4fa9b353a998a5f3a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/284 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abae3b2babfd4f23a88a0e2123b94a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/19.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc169f18f1ad4020b3f9e95e3e3e7c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/55.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d234b4541bba4ccfb05110a6d9571659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- 2) Chargement du modèle prêt à l'emploi -----\n",
    "model = SentenceTransformer(\n",
    "    \"infly/inf-retriever-v1-1.5b\",   # ↙ modèle E5 + pooling\n",
    "    device=device, \n",
    "    token=os.getenv(\"HF_token\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b3de4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3) Lecture du JSON source -----\n",
    "with open(\"/Users/oussa/Desktop/Github_perso/Advanced_RAG/data_chunks/chunks_aml_5_strat_2.json\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3090777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 4) Génération des embeddings -----\n",
    "for doc in docs:\n",
    "    emb = model.encode(\n",
    "        doc[\"page_content\"],\n",
    "        normalize_embeddings=True              # distance cosine déjà ok\n",
    "    ).tolist()                                 # .tolist() pour JSON\n",
    "\n",
    "    doc[\"embedding\"] = emb                    # ajoute le vecteur\n",
    "    # petite trace du modèle et de la dimension\n",
    "    doc.setdefault(\"metadata\", {})[\"embedding_model_name\"] = (\n",
    "        \"infly/inf-retriever-v1-1.5b\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 5) Sauvegarde -----\n",
    "with open(\"documents_with_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅  Embeddings ajoutés dans documents_with_embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664109a",
   "metadata": {},
   "source": [
    "Conclusion : \n",
    "- Working : ok\n",
    "- Result : None\n",
    "- Taking time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce75ff",
   "metadata": {},
   "source": [
    "## 2 - Testing the most efficient model : snowflake-arctic-embed-l-v2.0 (0.5 B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b4539",
   "metadata": {},
   "source": [
    " Even if this model is ranked at the 5th position it is the lighetest and most efficient one without a significant drop in performance (only 0.5 B param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab8f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_05B = SentenceTransformer(\n",
    "    \"infly/inf-retriever-v1-1.5b\",   # ↙ modèle E5 + pooling\n",
    "    device=device, \n",
    "    token=os.getenv(\"HF_token\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe1cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\",token =os.getenv('HF_token'))\n",
    "model= AutoModel.from_pretrained(\"intfloat/multilingual-e5-large-instruct\", token=os.getenv('HF_token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "    get_detailed_instruct(task, '南瓜的家常做法')\n",
    "]\n",
    "documents = [\n",
    "    \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day...\",\n",
    "    \"1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法:...\"\n",
    "]\n",
    "input_texts = queries + documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11763f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m batch_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(input_texts, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4df1d",
   "metadata": {},
   "source": [
    "Définir la fonction de pooling :\n",
    "Cette fonction effectue une moyenne pondérée par le masque d’attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdcd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6b788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0202,  0.0112, -0.0451,  ..., -0.0266, -0.0423,  0.0389],\n",
       "        [ 0.0486,  0.0594,  0.0040,  ..., -0.0214, -0.0263,  0.0198],\n",
       "        [ 0.0198, -0.0023, -0.0366,  ..., -0.0368, -0.0272,  0.0254],\n",
       "        [ 0.0461,  0.0436,  0.0091,  ..., -0.0155, -0.0181,  0.0149]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ty/2g_hgfks4nl_84mz496fmn600000gn/T/ipykernel_82697/1230817599.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  similarity = torch.matmul(embeddings[0], embeddings[1].T)\n"
     ]
    }
   ],
   "source": [
    "similarity = torch.matmul(embeddings[0], embeddings[1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = embeddings.tolist()  \n",
    "embedding_array = embeddings.cpu().numpy()  # to tbl NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31e49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02018598,  0.01120441, -0.04514236, ..., -0.02658733,\n",
       "        -0.04230955,  0.03893524],\n",
       "       [ 0.04857629,  0.05938283,  0.00398616, ..., -0.02135831,\n",
       "        -0.0263372 ,  0.01975931],\n",
       "       [ 0.01981018, -0.00233509, -0.03661876, ..., -0.03679116,\n",
       "        -0.02720564,  0.02541652],\n",
       "       [ 0.04608595,  0.04364255,  0.00913067, ..., -0.01546213,\n",
       "        -0.01805959,  0.01493509]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583c668",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We choose to use the 5th one (multilingual-e5-large-instrc) since it performing well globally and still good on retreival , reranking and STS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a89d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Advanced_RAG)",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
